<!doctype html>
<html lang="en">
<meta charset="utf-8">
<!--
<meta http-equiv="content-language” content=”en, zh”>
-->
<script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
            fontCache: 'global'
        }
    };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<title>  - muyun_</title>
<link rel=" stylesheet" href="/static/style.css">
<link rel="stylesheet" href="/static/pygments.css">
<nav>
    <h1><small><a href="/index.html">_muyun</a></small></h1>
    <ul>
        <li><a href="/index.html"> Writing </a>

            <!--
        <li><a href="/mini-projects.html"> Mini-projects </a>
            -->
        <li><a href="/slides.html"> Slides </a>

            <!--
        <li><a href="/bookshelf.html"> Bookshelf </a>
        -->

            <!--
        <li><a href="/talks.html"> Talks </a>
            -->

        <li><a href="/about.html"> About </a>
    </ul>
</nav>

<section class="content">
    <!--
    <header>
        
<title> Summary about Clustering Algorithms</title>

      </header>
      -->
    
<article class="post">
    <header>
        <h1><strong>Summary about Clustering Algorithms</strong></h1>
    </header>
    <p class="body"><h4>Clustering - divide a set of objects into meaningful groups</h4>
<h4>Centroid-based partitioning</h4>
<ul>
<li>
<p>Objects in the same cluster should be similar to each other, while in different clusters should be dissimilar.</p>
</li>
<li>
<p>k-center: find the k center set with the <strong>smallest radius r*</strong></p>
<ul>
<li>NP-hard</li>
<li>
<p>an optimal k-circle: a 2-approximate k circle cover [1]</p>
<ul>
<li>
<p>returning a k-center set with radius at most 2 * r*</p>
</li>
<li>
<p>choose <strong>a random point</strong> firstly, then choose the MAX distance to the points</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>k-mean  </p>
<ul>
<li>
<p><strong>k random points</strong> as the initial centroid, form k clusters by assigning all points to the closest centroid</p>
<ul>
<li>the centroid is the <strong>average of all the coordinates of the points in this cluster</strong></li>
<li>terminate until the the centorid set don't update.  </li>
</ul>
</li>
<li>
<p>k-means alg always terminates</p>
<ul>
<li>only a finite number of centroid sets that can possibily be produced at the end of each round</li>
<li>after each round, the cost (the distance) of the centroid set is strictly lower than that of the old centroid set</li>
</ul>
</li>
<li>
<p>the accuracy guarantee [1]</p>
<ul>
<li>
<p>k-seeding : the seed choice <small>(David Arthur, Sergei Vassilvitskii: k-means++: the advantages of careful seeding. SODA 2007: 1027-1035.)</small> 
       each point is chosen as the centroid <strong>with a probability proportional to ( D(p)^2 )</strong>.</p>
</li>
<li>
<p>if 100%, that's k-center</p>
</li>
<li>
<p>this gives the fact that the initial centroid set is picked too arbitrarily.
       By doing so more carefully, we can significantly improve the approximation ratio.</p>
</li>
</ul>
</li>
<li>
<p>the limitation of k-mean</p>
<ul>
<li>differing sizes, differing density, Non-globular shapes</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4>Hierarchical Methods</h4>
<ul>
<li>
<p>Why</p>
<ul>
<li>when a clustering needs, different users can explore the hierarchy to obtain <strong>various</strong> clustering results efficiently</li>
</ul>
</li>
<li>
<p>How: the agglomerative method</p>
<ul>
<li>merge the most similar two clusters until only one cluster is left</li>
</ul>
</li>
<li>
<p>Given a dendrogram (the merging history can be represented as a tree), we can obtain k clusters</p>
<ul>
<li>
<p>the alg:</p>
<ul>
<li><strong>binary search tree (BST)</strong> T is used to store the distances of all pairs of the current clusters</li>
<li>each time, remove the smallest cluster-pair distance from T, and merge them into a new cluster</li>
<li>O(n^2 * log n)</li>
</ul>
</li>
<li>
<p>distance function is the key</p>
<ul>
<li>distance graph G(V,E) (TODO)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4>Density-based</h4>
<ul>
<li>TODO</li>
</ul>
<h4>reference</h4>
<ul>
<li><a href="http://www.cse.cuhk.edu.hk/~taoyf/course/cmsc5724/spr15/cmsc5724.html">Data Mining and Knowledge Discovery</a></li>
<li><a href="http://www.cs.ubc.ca/research/flann/">FLANN lib</a></li>
</ul></p>
</article>




</section>


<div class="footer">
    <ul>
        <small>&copy; 2022 <a href="/index.html"></a>_muyun</small>
    </ul>
</div>